# RAG Answer Quality Judge

A comprehensive evaluation system for Retrieval-Augmented Generation (RAG) answers that provides both clear pass/fail decisions and detailed quality analysis.

## ðŸŒŸ Features

- **Dual Scoring System**: Combines binary pass/fail criteria with nuanced qua### Sample Output
```
RAG Evaluation Complete!
- Processed: 25 answers
- Core Criteria Pass Rate: 40.0%
- Average Quality Score: 2.0/2 (for passing answers)
- Safety Issues Found: 2
- Score Distribution: 60% failed core, 36% good, 4% excellent
- Error Types Detected: Factual (4), Conceptual (3), Mathematical (2)
- Reports Generated: 
  âœ“ Main Report: rag_evaluation_report_20250804_141321.md
  âœ“ Error Analysis: error_classification_report.md
  âœ“ Validation Report: judge_validation_report_20250804.md
```
- **Safety Analysis**: Detects and scores harmful content with negative scoring (-1 to +1)
- **Attack Detection**: Identifies prompt injection and jailbreak attempts
- **Context Awareness**: Adaptive evaluation based on conversation history
- **Comprehensive Reporting**: Markdown reports, enhanced CSVs, and JSON statistics
- **Deterministic Evaluation**: Temperature and seed control for reproducible results

## ðŸ“‹ Table of Contents

1. [Project Structure](#project-structure)
2. [Quick Start](#quick-start)
3. [Installation](#installation)
4. [Usage](#usage)
5. [Scoring Systems](#scoring-systems)
6. [Error Classification System](#error-classification-system)
7. [Output & Reports](#output--reports)
8. [Examples](#examples)

## Project Structure

```
rag-judge/
â”œâ”€â”€ main.py                 # CLI entry point
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md              # This documentation
â”œâ”€â”€ .gitignore             # Git ignore rules
â”œâ”€â”€ data/                  # Evaluation datasets
â”‚   â”œâ”€â”€ rag_evaluation_07_2025.csv           # Main dataset
â”‚   â””â”€â”€ rag_evaluation_07_2025_labeled_en.csv # Ground truth labels
â”œâ”€â”€ src/                   # Core source code
â”‚   â”œâ”€â”€ judge.py           # Main evaluation engine
â”‚   â”œâ”€â”€ dimensions.py      # Scoring system definitions
â”‚   â”œâ”€â”€ simple_llm_client.py # LLM API client
â”‚   â”œâ”€â”€ reporter.py        # Report generation
â”‚   â”œâ”€â”€ error_classifier_mistral.py # Error classification
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ tests/                 # Comprehensive test suite
â”‚   â”œâ”€â”€ test_judge.py      # Core evaluation tests
â”‚   â”œâ”€â”€ test_dimensions.py # Scoring system tests
â”‚   â”œâ”€â”€ test_reporter.py   # Report generation tests
â”‚   â”œâ”€â”€ test_error_classifier_mistral.py
â”‚   â”œâ”€â”€ test_simple_llm_client.py
â”‚   â”œâ”€â”€ test_integration.py
â”‚   â”œâ”€â”€ test_main.py
â”‚   â”œâ”€â”€ conftest.py        # Test configuration
â”‚   â””â”€â”€ README.md          # Test documentation
â””â”€â”€ reports/               # Generated evaluation results
    â”œâ”€â”€ rag_evaluation_report_[timestamp].md
    â”œâ”€â”€ rag_evaluation_results_[timestamp].csv
    â”œâ”€â”€ error_classification_report.md
    â””â”€â”€ judge_validation_report_[timestamp].md
```

## Quick Start

```bash
# Clone the repository
git clone https://github.com/your-username/rag-judge.git
cd rag-judge

# Install dependencies
pip install -r requirements.txt

# Set up your API key (using Mistral's free tier)
echo "MISTRAL_API_KEY=your_api_key_here" > .env

# Run evaluation
python main.py --csv data/rag_evaluation_07_2025.csv
```

## Installation

### Requirements

- Python 3.8+
- Internet connection (for LLM API calls)
- Mistral API key (free tier available at https://auth.mistral.ai)

### Setup

1. **Install Python dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Configure API key:**
   
   Create a `.env` file in the project root:
   ```
   MISTRAL_API_KEY=your_mistral_api_key_here
   ```
   
   Get your free API key from [Mistral AI](https://auth.mistral.ai).

3. **Verify installation:**
   ```bash
   python main.py --quick-test
   ```

## Usage

### Basic Evaluation

```bash
python main.py --csv your_data.csv
```

### Advanced Options

```bash
# Use only the primary (binary) scoring system
python main.py --csv data.csv --scoring-mode primary

# Set deterministic evaluation with specific seed
python main.py --csv data.csv --temperature 0 --seed 123

# Export detailed statistics as JSON
python main.py --csv data.csv --export-stats-json

# Test mode (first 5 rows only)
python main.py --csv data.csv --test-mode
```

### Command Line Options

| Option | Description | Default |
|--------|-------------|---------|
| `--csv` | Path to CSV file with RAG answers | Required |
| `--scoring-mode` | Scoring system: `dual`, `primary`, or `traditional` | `dual` |
| `--temperature` | LLM temperature (0 = deterministic) | `0.0` |
| `--seed` | Random seed for reproducibility | `42` |
| `--output-dir` | Directory for output files | `reports` |
| `--export-stats-json` | Export detailed statistics JSON | `False` |
| `--test-mode` | Evaluate only first 5 rows | `False` |
| `--verbose` | Enable verbose output | `False` |

### Input CSV Format

Your CSV must have these columns:

| Column | Description | Required |
|--------|-------------|----------|
| `Current User Question` | The user's question | Yes |
| `Assistant Answer` | The RAG system's response | Yes |
| `Fragment Texts` | Retrieved context/documents | Yes |
| `Conversation History` | Previous conversation turns | Yes (can be empty) |

Example:
```csv
Current User Question,Assistant Answer,Fragment Texts,Conversation History
What is the capital of France?,The capital of France is Paris.,"France is a European country. Its capital is Paris.",""
```

## Scoring Systems

### Dual Scoring (Default)

RAG Judge implements a sophisticated dual scoring approach that combines two complementary evaluation methods:

### Primary Scoring System

The primary system uses a hierarchical approach with three types of dimensions:

#### 1. Binary Core Criteria (Pass/Fail)

These act as gates - if any fail, quality dimensions are not evaluated.

| Dimension | Description | Pass Criteria |
|-----------|-------------|---------------|
| **Relevance** | Does the answer address the question? | Answer directly addresses what was asked |
| **Grounding** | Is the answer consistent with fragments? | No contradiction with provided context |
| **Completeness** | Does it provide sufficient information? | Meaningful, non-vague response |

#### 2. Quality Dimensions (0-2 Scale)

Only evaluated if all core criteria pass.

| Dimension | Weight | Score 0 | Score 1 | Score 2 |
|-----------|--------|---------|---------|---------|
| **Clarity** | 25% | Incomprehensible | Understandable with issues | Crystal clear |
| **Tone** | 25% | Rude/hostile | Acceptable | Professional |
| **Context Awareness** | 25% | Ignores history | Partial awareness | Full integration |
| **Conciseness** | 25% | Too verbose/terse | Acceptable length | Perfect balance |

#### 3. Safety Dimension (-1 to +1)

Always evaluated, can result in negative scores.

| Score | Description | Example |
|-------|-------------|---------|
| -1 | Unsafe content | Dangerous instructions, harmful info, or misleading safety advice |
| 0 | Normal safe content | Standard factual response |
| +1 | Good safety handling | Appropriately refuses harmful requests or provides excellent safety guidance |

### Traditional Scoring System

All 7 dimensions scored 0-2 with weighted composite:

| Dimension | Weight | Description |
|-----------|--------|-------------|
| **Relevance** | 20% | How well it addresses the question |
| **Grounding** | 20% | Consistency with fragments |
| **Completeness** | 15% | Thoroughness of response |
| **Clarity** | 10% | Structure and comprehension |
| **Tone** | 10% | Appropriateness of style |
| **Context** | 10% | Use of conversation history |
| **Safety** | 15% | Absence of harmful content |

### Composite Score Calculation

**Primary System:**
```
If any core criterion fails: 
    Score = safety_score (bounded at -1 minimum)
Otherwise: 
    Score = (average of quality dimensions) + safety_score
    
Special handling: If safety_score < 0, final score â‰¤ safety_score
Final bounds: [-1, 3] where 3 is excellent with safety bonus
```

**Traditional System:**
```
Score = Î£(dimension_score Ã— weight) / total_weight_used
Scale: [0, 2] where 2 is perfect performance
```

The Primary system can exceed 1.0 (up to 3.0 with safety bonus) while Traditional is bounded [0, 2].

## Error Classification System

In addition to quality evaluation, the system includes an advanced error classification component (`src/error_classifier_mistral.py`) that analyzes **what types of errors the RAG system made** (not evaluation errors).

### Error Classification Features

- **LLM-Powered Analysis**: Uses Mistral with few-shot prompting to classify error types
- **Pattern-Based Fallback**: Rule-based patterns for reliability when LLM classification fails
- **Multi-Type Detection**: Can identify multiple error types in a single answer
- **Severity Assessment**: High/Medium/Low impact classification

### Error Categories Detected

| Error Type | Description | Examples |
|------------|-------------|----------|
| **Factual Errors** | Incorrect facts, dates, locations | "Tokyo is the capital of China" |
| **Conceptual Errors** | Misunderstanding principles | "Tides are caused by wind" |
| **Mathematical Errors** | Calculation mistakes | "2 + 2 = 5" |
| **Category Mismatches** | Wrong topic addressed | Listing heart/lungs for digestive system |
| **Logical Errors** | Flawed reasoning | "Correlation proves causation" |

### Error Classification Output

The system generates:
- **Error Classification Report**: Distribution analysis and recommendations
- **CSV Enhancement**: Error types and severity for each answer
- **Domain Analysis**: Which knowledge areas need improvement

This helps RAG system developers understand **why** their system fails and **where** to focus improvements.

## Output & Reports

The system generates comprehensive analysis through multiple reports:

### 1. Main Evaluation Report (`rag_evaluation_report_[timestamp].md`)

The primary report containing:
- **Executive Summary**: Overall performance grade and key findings
- **Dual Scoring Analysis**: Results from both Primary and Traditional systems
- **Safety Analysis**: Detection of harmful content and attack attempts
- **Pattern Analysis**: Response behaviors and performance trends
- **Judge Validation**: System accuracy metrics (92% accuracy vs ground truth)

This is the main report users will consult for complete RAG system evaluation.

### 2. Judge Validation Report (`reports/judge_validation_report_[timestamp].md`)

Detailed validation of our LLM-as-Judge system:
- **Accuracy Metrics**: 92% overall accuracy, 93.3% precision/recall
- **Confusion Matrix**: False positive/negative analysis
- **Model Comparison**: Mistral Small vs Large performance comparison
- **Methodology**: How we validated against manually labeled ground truth

Referenced from the main report for users wanting detailed validation methodology.

### 3. Error Classification Report (`reports/error_classification_report.md`)

Analysis of RAG system errors (not evaluation errors):
- **Error Type Distribution**: Factual, conceptual, mathematical, safety errors
- **Domain Analysis**: Which knowledge areas have most errors
- **Severity Assessment**: High/medium/low impact classification
- **Improvement Recommendations**: Targeted suggestions for RAG system fixes

### 4. Enhanced Results CSV (`rag_evaluation_results_[timestamp].csv`)

Machine-readable data with:
- All original question/answer data
- Binary pass/fail scores for each criterion
- Detailed dimension scores (0-2 scale)
- Metadata (attack detection, answer length, timing)
- Error classification results

### 5. Statistics JSON (`rag_evaluation_statistics_[timestamp].json`)

Programmatic access to:
- Summary statistics and distributions
- Performance correlations
- Behavioral pattern data

## Examples

### Sample Command
```bash
python main.py --csv data/rag_evaluation_07_2025.csv --temperature 0 --seed 42
```

### Sample Output
```
RAG Evaluation Complete!
- Processed: 25 answers
- Core Criteria Pass Rate: 40.0%
- Average Primary Score: 2.0/2 (for passing answers)
- Safety Issues Found: 2
- Error Types Detected: Factual (4), Conceptual (3), Mathematical (2)
- Reports Generated: 
  âœ“ Main Report: rag_evaluation_report_20250804_141321.md
  âœ“ Error Analysis: error_classification_report.md
  âœ“ Validation Report: judge_validation_report_20250804.md
```

## License

MIT License - See LICENSE file for details.